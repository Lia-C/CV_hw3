{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a80ee45a7dde763ed37979dfddb89bea",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "## Neural Networks for Recognition - Assignment 3\n",
    "    Instructor: Kris                          TAs: Arka, Jinkun, Rawal, Rohan, Sheng-Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d23ab52a5796705fd330208f55a1aca",
     "grade": false,
     "grade_id": "cell-ee45598a54db40ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Theory Questions (45 points)\n",
    "**Grading**:  \n",
    "- The theory part consists of 7 questions.\n",
    "- Please add your answers to the writeup (submitted as pdf to HW3:PDF). Insert images whenever necessary.\n",
    "- Show all your work to obtain full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From coding qs, writeups:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.1 (3 points, write-up)\n",
    "Why is it not a good idea to initialize a network with all zeros? If you imagine that every layer has weights and biases, what can a zero-initialized network output after training?\n",
    "\n",
    "Since the weights are multipliers at each layer, if you initialize a network with all zeroes, it may just an output vector of all zeros as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.3 (2 points, write-up)\n",
    "Why is it a good practice to initialize the parameters using random numbers? Explain the intuition behind scaling the initializations depending on layer size (see near Fig 6 in [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))?\n",
    "\n",
    "It is good practice to initialize the parameters with random numbers because if you initialize them all with the same constant, the network is not incentivized to distinguish between different weights in training and backprop. \n",
    "\n",
    "In Xavier initialization, we also scale the random initializations depending on the layer size. Otherwise, in the standard initialization the variance of the back-propagated\n",
    "gradient is dependent on the layer, and decreasing. This causes the back-propagated gradients' effect to diminish the further up in the network we get.\n",
    "\n",
    "Instead, we want to maintain the activation variances and back-propagated gradients variance as one moves up or down the network, and Xavier initialization achieves that via scaling by the layer size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory qs, writeups:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "331415a2eb1c44f27d792e9bfe06b47a",
     "grade": false,
     "grade_id": "theory_q2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1 (3 points)\n",
    "\n",
    "The softmax function can be defined as, $softmax(x_i)= \\frac{1}{S} s_i$ where $s_i = e^{x_i}$ , $S=\\sum_i s_i$. Using this definition, please answer Q1.1, Q1.2 and Q1.3 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d5739e851b6472a72a8dab400606ed0",
     "grade": false,
     "grade_id": "cell-82ce863ef815b3b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.1 (1 point)\n",
    "Let $x \\in \\mathbb{R}^d$, what are the properties of $softmax(x)$, specifically, what is the range of each element in $softmax(x)$? What is the sum of all elements in $softmax(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61aacdeda627affd5fc5845996f1d72d",
     "grade": true,
     "grade_id": "theory_q2_ans",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The range of each element in  $softmax(x)$ = [0,1].\n",
    "\n",
    "The sum of all elements in  $softmax(x)$ = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e601d9a474cd8238bfd928279e38c4d",
     "grade": false,
     "grade_id": "cell-11b4b5748e0010bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2 (1 point)\n",
    "”Softmax takes an arbitrary real valued vector $x$ and turns it into a ___”. **Please fill in the blank using an appropriate word/phrase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed91437533a9e58883df69f77ea17eac",
     "grade": true,
     "grade_id": "cell-433a8c3d311f4d7a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "705218fbf3e7d15fbc223f44a096b08a",
     "grade": false,
     "grade_id": "cell-db431f37e7386bd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.3 (1 point)\n",
    "Let $x \\in \\mathbb{R}^d$, assume $v = softmax(softmax(... softmax(x)))$ where the softmax function is applied to $x$ recursively $N$ times. What is the value of $v$ as a function of $d$ $\\forall x \\in \\mathbb{R}^d$, in the limit $N \\rightarrow \\infty$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply softmax to x recursively, each successive softmax will smooth out the input probability distribution a little more. \n",
    "\n",
    "Thus, in the limit $N \\rightarrow \\infty$, the result will converge to a uniform probability distribution. So $ v=softmax^N(x)$ will approach $ones(n)/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(X: np.ndarray):\n",
    "    \"\"\"\n",
    "    A softmax function.\n",
    "    \n",
    "    [input]\n",
    "    * X -- input data [N x D]\n",
    "    \n",
    "    [output]\n",
    "    * res -- values after softmax\n",
    "    \"\"\"\n",
    "    res = None\n",
    "    \n",
    "    ## compute res using X\n",
    "    # YOUR CODE HERE\n",
    "    # shifted_X  = X - np.max(X, axis=1)\n",
    "\n",
    "    # # the inputs to each of these is a single x_vector, Nx1\n",
    "    # softmax_func = np.vectorize(lambda x: np.exp(x)/np.sum(np.exp(x)))\n",
    "    # # softmax_denom_func =  np.vectorize(lambda x: np.sum(np.exp(x)))\n",
    "\n",
    "    # res = np.apply_along_axis(softmax_func, 1, shifted_X)\n",
    "\n",
    "    max_X = np.max(X, axis=1).reshape((-1,1))\n",
    "    e_X = np.exp(X - max_X)\n",
    "    softmax_denom = np.sum(e_X, axis=1, keepdims=True)\n",
    "    res = e_X / softmax_denom\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0.1, 0.1, 0.2, 0.4, 0.2]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17984962, 0.17984962, 0.19876458, 0.2427716 , 0.19876458]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19595803, 0.19595803, 0.19969984, 0.20868427, 0.19969984]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.2, 0.2, 0.2, 0.2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(softmax(softmax(softmax(softmax(softmax(softmax(softmax(softmax(softmax(softmax(softmax(softmax(softmax(softmax(softmax(x))))))))))))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b13f448e7c97c97f5029349d1b16ce0",
     "grade": false,
     "grade_id": "theory_q1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2 (4 points)\n",
    "Prove that softmax is invariant to translation, that is \n",
    "$$softmax(x) = softmax(x + c) \\qquad \\forall c \\in \\mathbb{R}$$\n",
    "Again, softmax is defined as, for each index $i$ in a vector $x$.\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n",
    "Often we use $c = − \\max x_i$. Why is that a good idea? (Tip: consider the range of values that numerator will have with $c = 0$ and $c = − \\max x_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd84960af9fd395dabf70fa038e25a73",
     "grade": true,
     "grade_id": "theory_q1_ans",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$softmax(x_i + c) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}   $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\frac{e^{x_i + c}}{\\sum_j e^{x_j + c}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\frac{e^{x_i}e^c }{\\sum_j e^{x_j} e^c} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\frac{e^c}{e^c} \\frac{e^{x_i}}{\\sum_j e^{x_j}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\frac{e^{x_i}}{\\sum_j e^{x_j}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$=  softmax(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use $c = -\\max x_i$ to get a stable implementation of $softmax$.\n",
    "\n",
    "This is because overflow can occur when computing the exponent if the values of $x$ are large, resulting in NaNs. \n",
    "\n",
    "By offsetting $x$ by the maximum value, we only have to calculate the exponent of values equal to or smaller than 0, which means we get the maximum precision of our float values.\n",
    "\n",
    "(I referred to https://michielstock.github.io/posts/2021/2021-03-20-softmax/ for intuition on this answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba93aa4dd47896315d502eafc29b2c6e",
     "grade": false,
     "grade_id": "cell-38104f92a19a284a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3 (3 points)\n",
    "Show that the functions represented by a multi-layer fully-connected neural networks without a non-linear activation function are linear functions of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30381b92827e58e05fae85190eb15704",
     "grade": true,
     "grade_id": "cell-48454d1281c0524e",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The output of a particular node in a particular layer is $ f(W*X + b)$ where $f$ is the activation function. Since $W*X + b$ is already linear in $X$, if $f$ is linear too, then this whole term is linear in $X$ as well.\n",
    "\n",
    "If this is the case, then the output of each node is linear in its input $X$. \n",
    "\n",
    "Since the output of one layer (which we know to be linear) is the input to the subsequent layer (as $X$), then across all layers, the final output of the last layer is linear in the initial input to the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceef2df72991ea5b48aeb0ac12155489",
     "grade": false,
     "grade_id": "cell-23409e49f2f8eb65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4 (4 points) \n",
    "Given the sigmoid activation function $\\sigma(x) = \\frac{1}{1+e^{-x}}$ , derive the gradient of the sigmoid function and show that it can be written solely as a function of $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial }{\\partial x} \\sigma(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\frac{\\partial }{\\partial x}  \\frac{1}{1+e^{-x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\frac{\\partial }{\\partial x}  (1+e^{-x})^{-1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= -(1+e^{-x})^{-2}(-e^{-x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\frac{e^{-x}}{(1+e^{-x})^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\frac{e^{-x}}{1+e^{-x}} \\cdot \\frac{1}{1+e^{-x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\frac{(1 + e^{-x}) -1 }{1+e^{-x}} \\cdot \\frac{1}{1+e^{-x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$=  (\\frac{1 + e^{-x}}{1+e^{-x}} -  \\frac{1}{1+e^{-x}}  ) \\cdot \\frac{1}{1+e^{-x}}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$=  (1 -  \\frac{1}{1+e^{-x}}  ) \\cdot \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$=  (1 -  \\sigma(x)  ) \\cdot \\sigma(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$=  (1 -  \\sigma(x)  ) \\cdot \\sigma(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab63520ddff815e9419408b34d681af4",
     "grade": false,
     "grade_id": "cell-b4ba2548034e7814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5 (12 points WriteUp)\n",
    "\n",
    "Given $y = W x + b$ (or $y_j = \\sum_{i=1}^d  x_{i} W_{ji} + b_j$), and the gradient of some loss $J$ with respect $y$, show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$. Be sure to do the derivatives with scalars and re-form the matrix form afterwards. Here are some helpful notations.\n",
    "$$ \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{k \\times 1} \\quad W \\in \\mathbb{R}^{k \\times d} \\quad x \\in \\mathbb{R}^{d \\times 1} \\quad b \\in \\mathbb{R}^{k \\times 1}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J}{\\partial W} = \\frac{\\partial J}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W} = \\delta \\cdot x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J}{\\partial x} = \\frac{\\partial J}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} = \\delta \\cdot W$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial y} \\cdot \\frac{\\partial y}{\\partial b} = \\delta \\cdot 1 = \\delta $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05edab7c7dad1246cddc3d17936b7b93",
     "grade": false,
     "grade_id": "cell-efa5d9ebad394fed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6 (15 points)\n",
    "\n",
    "We will find the derivatives for Conv layers now. Since most Deep Learning frameworks such as Pytorch, Tensorflow use cross-correlation in their respective \"convolution\" functions ([Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/convolution)), we will continue this abuse of notation. So the operation performed with the Conv Layer weights will be cross-correlation.\n",
    "    \n",
    "The input, $x$ is of shape $M\\times N$ with C channels. This will be *convolved* (actually cross-correlation) with $D$ number of $K\\times K$ filters, each with a bias term. The stride is 1 and there will be no padding. We know the gradient of some loss $J$ with respect to the output $y$, which will have $D$ channels. Show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$.\n",
    "\n",
    "The dimensions and notation are as follows:\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{D\\times M_o \\times N_o}\n",
    "    \\quad\n",
    "    M_o = M-K+1\n",
    "    \\quad\n",
    "    N_o = N-K+1\n",
    "$$\n",
    "$$\n",
    "    x \\in \\mathbb{R}^{C\\times M \\times N}\n",
    "    \\quad\n",
    "    W \\in \\mathbb{R}^{D\\times C \\times K \\times K}\n",
    "    \\quad\n",
    "    b \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "$x_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the input\n",
    "\n",
    "$y_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the output\n",
    "\n",
    "$W_{d, c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column, the $c^{th}$ channel of the kernel of the $d^{th}$ filter\n",
    "\n",
    "*For this question, you may compute the derivatives with scalars only. You don't need to re-form the matrix*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$, we need to express $y$ as a function of $W$ and $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3624c404829a26b2682c1f41740dc2af",
     "grade": true,
     "grade_id": "cell-b02303c52179085f",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "By the way, I'll be using the notation  $y_{d, i, j}$  instead of  $y_{c, i, j}$ since the first iterator ranges $1...D$ (not $1...C$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$y_{d, i, j} = b_d + \\sum_{c=1}^{C} \\sum_{k_i=1}^{K} \\sum_{k_j=1}^{K} x_{c, i+k_i, j+k_j} \\cdot W_{d, c, k_i, k_j}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for $\\frac{\\partial J}{\\partial W}$. In scalar form, this is:\n",
    "$$\\frac{\\partial J}{\\partial W_{d, c, k_i, k_j}} = \\frac{\\partial J}{\\partial y_{d,i,j}} \\cdot \\frac{\\partial y_{d,i,j}}{\\partial W_{d, c, k_i, k_j}} =  \\sum_{c=1}^{C} \\sum_{k_i=1}^{K} \\sum_{k_j=1}^{K} \\delta_{d,i,j} \\cdot x_{c, i+k_i, j+k_j}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for  $\\frac{\\partial J}{\\partial x}$. In scalar form, this is:\n",
    "$$\\frac{\\partial J}{\\partial x_{c, i+k_i, j+k_j}} = \\frac{\\partial J}{\\partial y_{d,i,j}} \\cdot \\frac{\\partial y_{d,i,j}}{\\partial x_{c, i+k_i, j+k_j}} =  \\sum_{c=1}^{C} \\sum_{k_i=1}^{K} \\sum_{k_j=1}^{K} \\delta_{d,i,j} \\cdot W_{d, c, k_i, k_j}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for  $\\frac{\\partial J}{\\partial b}$. In scalar form, this is:\n",
    "$$\\frac{\\partial J}{\\partial b_d} = \\frac{\\partial J}{\\partial y_{d,i,j}} \\cdot \\frac{\\partial y_{d,i,j}}{\\partial b_d} =   \\delta_{d,i,j} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeeba4956195062fd9e58077f0fa9f8a",
     "grade": false,
     "grade_id": "cell-775c17b4eb8e7758",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q7 (4 points)\n",
    "\n",
    "When the neural network applies the elementwise activation function (such as sigmoid), the gradient of the activation function scales the back-propagation update. This is directly from the chain rule, $\\frac{d}{d x} f(g(x)) = f'(g(x)) g'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b380b2b0ebb74f689d00c30fab3fdf26",
     "grade": false,
     "grade_id": "cell-4c0ef8ab732dda1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1 (1 point)\n",
    "Consider the sigmoid activation function for deep neural networks. Why might it lead to a \"vanishing gradient\" problem if it is used for many layers (consider plotting the $\\sigma'(x)$ in Q1.4)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a graph of $\\sigma(x)$ and its derivative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "160f0d9b648792cd8f4b786a13d15578",
     "grade": true,
     "grade_id": "cell-cecc47088410bfc3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"sigmoid_graph.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when $|x|$ is large, the derivative of the sigmoid gets squashed down close to 0.\n",
    "\n",
    "When we do backprop, because of the chain rule, we multiply the derivatives of each layer down to the first layer. So if the derivatives are close to 0, and we are multiplying many of them together (in the case of a network with many layers), the gradient gets exponentially small.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99bef598365d14fdd07e298ac59e55c5",
     "grade": false,
     "grade_id": "cell-c01f226eb7134484",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.2 (1 point)\n",
    "Often it is replaced with $\\tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$. What are the output ranges of both $\\tanh$ and sigmoid? Why might we prefer $\\tanh$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6f866c6c71afe439ec39b1b4c5b8012",
     "grade": true,
     "grade_id": "cell-359e85eddc00f567",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The output range of $\\tanh$ is [-1,1]. The output range of $\\sigma(x)$ is [0,1].\n",
    "\n",
    "We might prefer to use $\\tanh$ since its output range is centered over 0, while $\\sigma(x)$ is centered at 0.5. Convergence generally occurs faster if average of each input variable is close to 0. \n",
    "\n",
    "For instance. consider a large negative input to the activation functions. In $\\sigma(x)$, this will map to a value close to 0. This means that updates of the weights will be slow in the backprop step. \n",
    "\n",
    "On the other hand with $\\tanh$, a large negative input will still be mapped to a strongly-negative output. It is only input values that are near zero that will remain close to 0.  So, with $\\tanh$ the network is less likely to get stuck during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "889dabb3ca92c8a774f432172f58867e",
     "grade": false,
     "grade_id": "cell-adfb2179efada3bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.3 (1 point)\n",
    "Why does $\\tanh(x)$ have less of a vanishing gradient problem? (plotting the derivatives helps! for reference: $\\tanh'(x) = 1 - \\tanh(x)^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5832ee3b18a8a57b28840e7ab6cb7f9e",
     "grade": true,
     "grade_id": "cell-c9f65d287cf03f87",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Here is a graph of the derivatives. $\\tanh'(x)$ is green and $\\sigma'(x)$ is black:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"vanishing_grads.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although both will have the vanishing gradient problem (since as $|x|$ gets very large, both derivatives approach 0), $\\tanh'(x) $ will have slightly less of a vanishing gradient problem. This is because $\\tanh'(x) $ ranges [0,1] while $\\sigma'(x)$ ranges [0, 0.25], so with $\\tanh(x)$ the updates to the weights are generally larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55afcde21a8360b35531aea4b919dd84",
     "grade": false,
     "grade_id": "cell-a1326ba9e015ed2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.4 (1 point)\n",
    "$\\tanh$ is a scaled and shifted version of the sigmoid. Show how $\\tanh(x)$ can be written in terms of $\\sigma(x)$. (*Hint: consider how to make it have the same range*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "965e794e6e31a8d771c8653489b4d1d6",
     "grade": true,
     "grade_id": "cell-1608d4ea95217d89",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$\\tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ = \\frac{(2-1) - e^{-2x}}{1+e^{-2x}} $  , substituting $2-1$ in for 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ = \\frac{2-1-e^{-2x}}{1+e^{-2x}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ = \\frac{2}{1+e^{-2x}} - \\frac{1+e^{-2x}}{1+e^{-2x}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ = \\frac{2}{1+e^{-2x}} - 1 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ = 2\\cdot \\frac{1}{1+e^{-2x}} - 1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ = 2\\cdot \\sigma(2x) - 1$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cv_hw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3baf386052658501891e8c70ba6a2b62e900282515b79869d0a53bf79ee7c064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
